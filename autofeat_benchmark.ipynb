{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import FeatureSelector, AutoFeatRegression\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"boston\", \"diabetes\", \"concrete\", \"forest_fires\", \"wine_quality\", \"airfoil\"]\n",
    "\n",
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "\n",
    "    elif name == \"concrete\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "        # Cement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Blast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Water (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Superplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Coarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fine Aggregate (component 7)    -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Age -- quantitative -- Day (1~365) -- Input Variable\n",
    "        # Concrete compressive strength -- quantitative -- MPa -- Output Variable\n",
    "        df = pd.read_csv(os.path.join(datapath, \"concrete.csv\"))\n",
    "        X = df.iloc[:, :8].to_numpy()\n",
    "        y = df.iloc[:, 8].to_numpy()\n",
    "\n",
    "    elif name == \"forest_fires\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        # 1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "        # 2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "        # 3. month - month of the year: 'jan' to 'dec'\n",
    "        # 4. day - day of the week: 'mon' to 'sun'\n",
    "        # 5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "        # 6. DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "        # 7. DC - DC index from the FWI system: 7.9 to 860.6\n",
    "        # 8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "        # 9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "        # 10. RH - relative humidity in %: 15.0 to 100\n",
    "        # 11. wind - wind speed in km/h: 0.40 to 9.40\n",
    "        # 12. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "        # 13. area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "        # (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
    "        # --> first 4 are ignored\n",
    "        df = pd.read_csv(os.path.join(datapath, \"forest_fires.csv\"))\n",
    "        X = df.iloc[:, 4:12].to_numpy()\n",
    "        y = df.iloc[:, 12].to_numpy()\n",
    "        # perform transformation as they suggested\n",
    "        y = np.log(y + 1)\n",
    "\n",
    "    elif name == \"wine_quality\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "        # Input variables (based on physicochemical tests):\n",
    "        # 1 - fixed acidity\n",
    "        # 2 - volatile acidity\n",
    "        # 3 - citric acid\n",
    "        # 4 - residual sugar\n",
    "        # 5 - chlorides\n",
    "        # 6 - free sulfur dioxide\n",
    "        # 7 - total sulfur dioxide\n",
    "        # 8 - density\n",
    "        # 9 - pH\n",
    "        # 10 - sulphates\n",
    "        # 11 - alcohol\n",
    "        # Output variable (based on sensory data):\n",
    "        # 12 - quality (score between 0 and 10)\n",
    "        df_red = pd.read_csv(os.path.join(datapath, \"winequality-red.csv\"), sep=\";\")\n",
    "        df_white = pd.read_csv(os.path.join(datapath, \"winequality-white.csv\"), sep=\";\")\n",
    "        # add additional categorical feature for red or white\n",
    "        X = np.hstack([np.vstack([df_red.iloc[:, :-1].to_numpy(), df_white.iloc[:, :-1].to_numpy()]), np.array([[1]*len(df_red) + [0]*len(df_white)]).T])\n",
    "        y = np.hstack([df_red[\"quality\"].to_numpy(), df_white[\"quality\"].to_numpy()])\n",
    "\n",
    "    elif name == \"airfoil\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise\n",
    "        # This problem has the following inputs:\n",
    "        # 1. Frequency, in Hertz.\n",
    "        # 2. Angle of attack, in degrees.\n",
    "        # 3. Chord length, in meters.\n",
    "        # 4. Free-stream velocity, in meters per second.\n",
    "        # 5. Suction side displacement thickness, in meters.\n",
    "        # The only output is:\n",
    "        # 6. Scaled sound pressure level, in decibels.\n",
    "        units = {\"x001\": \"Hz\", \"x003\": \"m\", \"x004\": \"m/sec\", \"x005\": \"m\"}\n",
    "        df = pd.read_csv(os.path.join(datapath, \"airfoil_self_noise.tsv\"), header=None, names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"y\"], sep=\"\\t\")\n",
    "        X = df.iloc[:, :5].to_numpy()\n",
    "        y = df[\"y\"].to_numpy()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units\n",
    "\n",
    "def test_model(dataset, model, param_grid):\n",
    "    # load data\n",
    "    X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, iid=False)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(dataset, feateng_steps=3):\n",
    "    # load data\n",
    "    X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegression(verbose=1, feateng_steps=feateng_steps, units=units, featsel_w_thr=0.01)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5, iid=False)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "(506, 13)\n",
      "#### diabetes\n",
      "(442, 10)\n",
      "#### concrete\n",
      "(1030, 8)\n",
      "#### forest_fires\n",
      "(517, 8)\n",
      "#### wine_quality\n",
      "(6497, 12)\n",
      "#### airfoil\n",
      "(1503, 5)\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    X, y, _ = load_regression_dataset(dsname)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -25.427148426837693\n",
      "MSE on training data: 22.4278718761592\n",
      "MSE on test data: 20.5580503052922\n",
      "R^2 on training data: 0.7361592384229154\n",
      "R^2 on test data: 0.7484031841564716\n",
      "#### diabetes\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3043.14487668777\n",
      "MSE on training data: 2817.5756461735427\n",
      "MSE on test data: 3119.632550355442\n",
      "R^2 on training data: 0.541317737800587\n",
      "R^2 on test data: 0.38300930348673157\n",
      "#### concrete\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -110.34480414200297\n",
      "MSE on training data: 107.00865107837934\n",
      "MSE on test data: 110.56229503996865\n",
      "R^2 on training data: 0.6245955930727385\n",
      "R^2 on test data: 0.5643057266127824\n",
      "#### forest_fires\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.8969026573257473\n",
      "MSE on training data: 1.8469604643703812\n",
      "MSE on test data: 2.328893858372984\n",
      "R^2 on training data: 0.010801489514856932\n",
      "R^2 on test data: -0.0401625480215837\n",
      "#### wine_quality\n",
      "best params: {'alpha': 0.0001}\n",
      "best score: -0.5401265191014872\n",
      "MSE on training data: 0.5348196387905402\n",
      "MSE on test data: 0.5434554548609953\n",
      "R^2 on training data: 0.29251728914027897\n",
      "R^2 on test data: 0.3100144852264428\n",
      "#### airfoil\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -22.960476312553066\n",
      "MSE on training data: 22.6317043193984\n",
      "MSE on test data: 24.732769352718233\n",
      "R^2 on training data: 0.5173357362628234\n",
      "R^2 on test data: 0.5076580301932743\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rreg = Ridge()\n",
    "    params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    rreg = test_model(dsname, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'C': 100.0}\n",
      "best score: -13.598043246310898\n",
      "MSE on training data: 3.4469342608444284\n",
      "MSE on test data: 9.636188588435925\n",
      "R^2 on training data: 0.9594503764998732\n",
      "R^2 on test data: 0.8820688572255264\n",
      "#### diabetes\n",
      "best params: {'C': 10.0}\n",
      "best score: -3057.7070015538065\n",
      "MSE on training data: 2577.0841482085507\n",
      "MSE on test data: 3437.6800004513143\n",
      "R^2 on training data: 0.5804681274187382\n",
      "R^2 on test data: 0.32010692168648935\n",
      "#### concrete\n",
      "best params: {'C': 100.0}\n",
      "best score: -37.08377959823637\n",
      "MSE on training data: 18.997096173790347\n",
      "MSE on test data: 30.152373071635388\n",
      "R^2 on training data: 0.9333549806432162\n",
      "R^2 on test data: 0.8811781514521082\n",
      "#### forest_fires\n",
      "best params: {'C': 1.0}\n",
      "best score: -2.186927498252301\n",
      "MSE on training data: 1.8539927476295148\n",
      "MSE on test data: 3.12614582192515\n",
      "R^2 on training data: 0.007035126206362374\n",
      "R^2 on test data: -0.396242165322382\n",
      "#### wine_quality\n",
      "best params: {'C': 10.0}\n",
      "best score: -0.4640006945371349\n",
      "MSE on training data: 0.32390678929749517\n",
      "MSE on test data: 0.4638085434435221\n",
      "R^2 on training data: 0.5715219922060317\n",
      "R^2 on test data: 0.4111363245289218\n",
      "#### airfoil\n",
      "best params: {'C': 250.0}\n",
      "best score: -7.094189398840056\n",
      "MSE on training data: 5.457762290823167\n",
      "MSE on test data: 7.477589784074904\n",
      "R^2 on training data: 0.8836028086716045\n",
      "R^2 on test data: 0.8511476320667879\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    svr = test_model(dsname, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.462810946604932\n",
      "MSE on training data: 1.4186988960396048\n",
      "MSE on test data: 10.583239343137262\n",
      "R^2 on training data: 0.9833104719321342\n",
      "R^2 on test data: 0.8704785093673091\n",
      "#### diabetes\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3336.6571277553485\n",
      "MSE on training data: 2472.319475907154\n",
      "MSE on test data: 3268.607555103584\n",
      "R^2 on training data: 0.5975231076301993\n",
      "R^2 on test data: 0.35354551553768243\n",
      "#### concrete\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.701648337050347\n",
      "MSE on training data: 4.169688233206215\n",
      "MSE on test data: 27.527437198114896\n",
      "R^2 on training data: 0.9853720299949222\n",
      "R^2 on test data: 0.8915222703733743\n",
      "#### forest_fires\n",
      "best params: {'min_samples_leaf': 0.2}\n",
      "best score: -1.8836034096444383\n",
      "MSE on training data: 1.828520757317779\n",
      "MSE on test data: 2.2946977602510548\n",
      "R^2 on training data: 0.020677462012424486\n",
      "R^2 on test data: -0.024889417205844477\n",
      "#### wine_quality\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3930715087362109\n",
      "MSE on training data: 0.05242462959399654\n",
      "MSE on test data: 0.3478176153846154\n",
      "R^2 on training data: 0.9306504167557255\n",
      "R^2 on test data: 0.558401495004132\n",
      "#### airfoil\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.770457737476125\n",
      "MSE on training data: 0.4389576916890236\n",
      "MSE on test data: 3.316904702700349\n",
      "R^2 on training data: 0.9906383899294207\n",
      "R^2 on test data: 0.9339721576787678\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    rforest = test_model(dsname, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 91 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Generated a total of 64 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 17 new features selected.\n",
      "[AutoFeatRegression] Computing 16 new features.\n",
      "[AutoFeatRegression]    16/   16 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "21.166797229359744\n",
      "542.796203 * 1/x009\n",
      "93.728034 * 1/x010\n",
      "21.157874 * 1/x012\n",
      "-5.839516 * log(x012)\n",
      "2.403238 * 1/x007\n",
      "-1.294779 * 1/x006\n",
      "0.942274 * x003**3\n",
      "-0.560753 * x004**3\n",
      "-0.045817 * x000\n",
      "0.039244 * sqrt(x011)\n",
      "-0.018908 * x010\n",
      "0.013633 * x005**3\n",
      "0.001669 * x011\n",
      "0.001458 * exp(x005)\n",
      "-0.000483 * x007**3\n",
      "[AutoFeatRegression] Final R^2: 0.7795\n",
      "[AutoFeatRegression] Final dataframe with 29 feature columns (16 new).\n",
      "[AutoFeatRegression] Computing 16 new features.\n",
      "[AutoFeatRegression]    16/   16 new features ...done.\n",
      "autofeat new features: 16\n",
      "autofeat MSE on training data: 18.74459945141544\n",
      "autofeat MSE on test data: 16.778413251218016\n",
      "autofeat R^2 on training data: 0.7794891364625635\n",
      "autofeat R^2 on test data: 0.7946597422311679\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -16.364967715664353\n",
      "MSE on training data: 12.637387456162697\n",
      "MSE on test data: 15.288709329729038\n",
      "R^2 on training data: 0.8513341814511178\n",
      "R^2 on test data: 0.8128912747758548\n",
      "#### diabetes\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 70 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Generated a total of 50 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 16 new features selected.\n",
      "[AutoFeatRegression] Computing 14 new features.\n",
      "[AutoFeatRegression]    14/   14 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-657.6400354820338\n",
      "10999.225682 * x003**3\n",
      "-4912.429722 * x005**3\n",
      "2164.377248 * x000**2\n",
      "1950.828753 * x009**2\n",
      "1060.218715 * x002**2\n",
      "482.479370 * exp(x002)\n",
      "469.954953 * x008\n",
      "366.321008 * x003**2\n",
      "243.079589 * exp(x003)\n",
      "-204.652700 * x006\n",
      "69.339680 * exp(x009)\n",
      "-0.306720 * 1/x001\n",
      "-0.017583 * 1/x009\n",
      "0.015834 * 1/x002\n",
      "0.012191 * 1/x008\n",
      "-0.008905 * 1/x007\n",
      "[AutoFeatRegression] Final R^2: 0.5743\n",
      "[AutoFeatRegression] Final dataframe with 24 feature columns (14 new).\n",
      "[AutoFeatRegression] Computing 14 new features.\n",
      "[AutoFeatRegression]    14/   14 new features ...done.\n",
      "autofeat new features: 14\n",
      "autofeat MSE on training data: 2614.702339095961\n",
      "autofeat MSE on test data: 3196.71714720504\n",
      "autofeat R^2 on training data: 0.5743441403238326\n",
      "autofeat R^2 on test data: 0.36776376468272753\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -2914.3840553680543\n",
      "MSE on training data: 2572.1393630420866\n",
      "MSE on test data: 3208.0763468469822\n",
      "R^2 on training data: 0.581273105006234\n",
      "R^2 on test data: 0.36551717942443596\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Generated a total of 34 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 20 new features selected.\n",
      "[AutoFeatRegression] Computing 16 new features.\n",
      "[AutoFeatRegression]    16/   16 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-645.5800363498394\n",
      "-59500.556454 * 1/x003\n",
      "-54769.437924 * 1/x006\n",
      "27852.305249 * 1/x000\n",
      "-14618.978879 * 1/x005\n",
      "333.412390 * log(x000)\n",
      "-16.021359 * 1/x007\n",
      "11.751719 * sqrt(x007)\n",
      "-5.233870 * log(x007)\n",
      "-4.363989 * x003\n",
      "1.540142 * sqrt(x004)\n",
      "-0.862941 * x000\n",
      "-0.589442 * x007\n",
      "0.138692 * x001\n",
      "0.077737 * x002\n",
      "-0.058715 * x006\n",
      "-0.046148 * x004**2\n",
      "0.000942 * x004**3\n",
      "0.000472 * x007**2\n",
      "0.000023 * x003**3\n",
      "[AutoFeatRegression] Final R^2: 0.8737\n",
      "[AutoFeatRegression] Final dataframe with 24 feature columns (16 new).\n",
      "[AutoFeatRegression] Computing 16 new features.\n",
      "[AutoFeatRegression]    16/   16 new features ...done.\n",
      "autofeat new features: 16\n",
      "autofeat MSE on training data: 36.003226534540346\n",
      "autofeat MSE on test data: 40.001475024002175\n",
      "autofeat R^2 on training data: 0.8736946053570265\n",
      "autofeat R^2 on test data: 0.8423656673488993\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -43.58876145661876\n",
      "MSE on training data: 39.854784284443895\n",
      "MSE on test data: 44.395169114020675\n",
      "R^2 on training data: 0.8601826907755634\n",
      "R^2 on test data: 0.8250513799298086\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Generated a total of 38 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 5 new features selected.\n",
      "[AutoFeatRegression] Computing 5 new features.\n",
      "[AutoFeatRegression]     5/    5 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "0.000237163996712475\n",
      "3.784360 * 1/x004\n",
      "-0.829497 * sqrt(x007)\n",
      "0.121388 * log(x001)\n",
      "-0.006250 * x003\n",
      "0.005643 * x004\n",
      "0.000370 * x002\n",
      "0.000119 * exp(x006)\n",
      "0.000060 * x001\n",
      "[AutoFeatRegression] Final R^2: 0.0469\n",
      "[AutoFeatRegression] Final dataframe with 13 feature columns (5 new).\n",
      "[AutoFeatRegression] Computing 5 new features.\n",
      "[AutoFeatRegression]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 1.7796133627137953\n",
      "autofeat MSE on test data: 2.434022039059077\n",
      "autofeat R^2 on training data: 0.04687137510761463\n",
      "autofeat R^2 on test data: -0.08711633936685104\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.858023815306241\n",
      "MSE on training data: 1.8136662566746768\n",
      "MSE on test data: 2.364579699621341\n",
      "R^2 on training data: 0.02863326301283542\n",
      "R^2 on test data: -0.05610104836488272\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 84 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Generated a total of 63 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 7 new features selected.\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "79.31279448670543\n",
      "-78.526901 * x007\n",
      "1.055794 * sqrt(x009)\n",
      "-0.679206 * x004\n",
      "-0.523772 * log(x001)\n",
      "0.366087 * x008\n",
      "0.359086 * x011\n",
      "0.289216 * log(x005)\n",
      "-0.267860 * 1/x003\n",
      "-0.243649 * x001**3\n",
      "0.065356 * x000\n",
      "0.038796 * x003\n",
      "-0.004119 * x005\n",
      "0.003634 * x002\n",
      "-0.001056 * x006\n",
      "0.000667 * x010**3\n",
      "[AutoFeatRegression] Final R^2: 0.3145\n",
      "[AutoFeatRegression] Final dataframe with 19 feature columns (7 new).\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "autofeat new features: 7\n",
      "autofeat MSE on training data: 0.5182192618893671\n",
      "autofeat MSE on test data: 0.5240756958906768\n",
      "autofeat R^2 on training data: 0.3144769906910567\n",
      "autofeat R^2 on test data: 0.33461954319341614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'alpha': 0.001}\n",
      "best score: -0.5245556594266705\n",
      "MSE on training data: 0.5178094322818836\n",
      "MSE on test data: 0.5235956370832444\n",
      "R^2 on training data: 0.31501913114489033\n",
      "R^2 on test data: 0.3352290386367043\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 35 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Generated a total of 22 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 12 new features selected.\n",
      "[AutoFeatRegression] Computing 9 new features.\n",
      "[AutoFeatRegression]     9/    9 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "142.15178860186927\n",
      "-44636.867087 * x004**3\n",
      "-1094.731875 * 1/x000\n",
      "-113.021886 * 1/x003\n",
      "-36.801218 * x004\n",
      "-23.569898 * x002\n",
      "-0.225903 * sqrt(x000)\n",
      "-0.119485 * x001\n",
      "-0.007737 * x001**2\n",
      "0.003659 * 1/x004\n",
      "[AutoFeatRegression] Final R^2: 0.5723\n",
      "[AutoFeatRegression] Final dataframe with 15 feature columns (10 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 9 new features.\n",
      "[AutoFeatRegression]     9/    9 new features ...done.\n",
      "autofeat new features: 9\n",
      "autofeat MSE on training data: 20.0549528317371\n",
      "autofeat MSE on test data: 22.039830218400784\n",
      "autofeat R^2 on training data: 0.5722898767938877\n",
      "autofeat R^2 on test data: 0.5612649247165419\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -20.959599835748246\n",
      "MSE on training data: 20.07130370932988\n",
      "MSE on test data: 21.954096811348723\n",
      "R^2 on training data: 0.5719411631405368\n",
      "R^2 on test data: 0.5629715736527886\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11659 feature combinations from 2926 original feature tuples - done.\n",
      "[feateng] Generated a total of 2945 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 18 new features selected.\n",
      "[AutoFeatRegression] Computing 18 new features.\n",
      "[AutoFeatRegression]    18/   18 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "13.03771162815117\n",
      "53.637068 * 1/(x007*x012)\n",
      "12.696388 * 1/(x002*x007)\n",
      "-10.751187 * log(x004)/x007\n",
      "3.063977 * x005**3/x009\n",
      "1.772136 * x008/x012\n",
      "-1.469276 * x003**2*log(x004)\n",
      "-1.044754 * sqrt(x000)*x004**3\n",
      "0.297604 * x005**3/x010\n",
      "-0.281200 * x000*x004**3\n",
      "0.259411 * exp(x005)/x009\n",
      "0.026778 * x000**3*x003**3\n",
      "-0.005399 * x005**2*x012\n",
      "-0.000499 * x006**2/x008\n",
      "-0.000479 * exp(x005)*log(x000)\n",
      "-0.000060 * x010**3*sqrt(x012)\n",
      "0.000018 * x005**3*x011\n",
      "-0.000013 * x009**2/x008\n",
      "[AutoFeatRegression] Final R^2: 0.9046\n",
      "[AutoFeatRegression] Final dataframe with 31 feature columns (18 new).\n",
      "[AutoFeatRegression] Computing 18 new features.\n",
      "[AutoFeatRegression]    18/   18 new features ...done.\n",
      "autofeat new features: 18\n",
      "autofeat MSE on training data: 8.110708925394773\n",
      "autofeat MSE on test data: 13.531281572405062\n",
      "autofeat R^2 on training data: 0.9045858817268809\n",
      "autofeat R^2 on test data: 0.8343993079429846\n",
      "best params: {'alpha': 25.0}\n",
      "best score: -11.635043727392938\n",
      "MSE on training data: 9.042465592478933\n",
      "MSE on test data: 17.551409289604226\n",
      "R^2 on training data: 0.89362472633927\n",
      "R^2 on test data: 0.7851995386111988\n",
      "#### diabetes\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 2485 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 7041 feature combinations from 1770 original feature tuples - done.\n",
      "[feateng] Generated a total of 1781 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 6 new features selected.\n",
      "[AutoFeatRegression] Computing 6 new features.\n",
      "[AutoFeatRegression]     6/    6 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-222.14841217693024\n",
      "-1869.172747 * x006*Abs(x009)\n",
      "268.532445 * exp(x002)*exp(x003)\n",
      "241.193596 * exp(x002)*exp(x008)\n",
      "-161.040746 * exp(x001)*exp(x006)\n",
      "25.397181 * exp(x003)*exp(x008)\n",
      "10.316801 * Abs(x008)/x008\n",
      "5.719606 * x009\n",
      "-0.897140 * x005\n",
      "[AutoFeatRegression] Final R^2: 0.5437\n",
      "[AutoFeatRegression] Final dataframe with 16 feature columns (6 new).\n",
      "[AutoFeatRegression] Computing 6 new features.\n",
      "[AutoFeatRegression]     6/    6 new features ...done.\n",
      "autofeat new features: 6\n",
      "autofeat MSE on training data: 2803.1481080565054\n",
      "autofeat MSE on test data: 3018.77834824876\n",
      "autofeat R^2 on training data: 0.543666443444206\n",
      "autofeat R^2 on test data: 0.40295591687778676\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2987.860836302082\n",
      "MSE on training data: 2755.8792472252694\n",
      "MSE on test data: 3052.483323332176\n",
      "R^2 on training data: 0.5513614943462484\n",
      "R^2 on test data: 0.39628985742462786\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3422 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Generated a total of 873 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 81 new features selected.\n",
      "[AutoFeatRegression] Computing 81 new features.\n",
      "[AutoFeatRegression]    81/   81 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "127.6988826596997\n",
      "-12739885.201232 * 1/(x003*x006)\n",
      "-1085015.367938 * 1/(x000*x003)\n",
      "6115.637421 * 1/(x000*x007)\n",
      "-1044.494547 * sqrt(x007)/x006\n",
      "-476.611154 * sqrt(x004)/x000\n",
      "418.105248 * log(x007)/x003\n",
      "384.399922 * sqrt(x002)/x005\n",
      "45.610063 * x000/x005\n",
      "-12.346441 * log(x007)/x007\n",
      "6.142597 * x007/x000\n",
      "1.570058 * x001/x000\n",
      "-0.983197 * x004/x007\n",
      "-0.574259 * sqrt(x001)/x007\n",
      "0.309737 * sqrt(x000)*sqrt(x004)\n",
      "0.179734 * x004**3/x000\n",
      "0.163724 * sqrt(x001)*log(x007)\n",
      "-0.091288 * sqrt(x001)*x004\n",
      "-0.062181 * x000/x007\n",
      "-0.023108 * x002/x007\n",
      "-0.014357 * x002*sqrt(x004)\n",
      "0.009761 * x001*sqrt(x004)\n",
      "0.008057 * sqrt(x002)*x007\n",
      "0.006248 * x001*sqrt(x002)\n",
      "0.004154 * x006*sqrt(x007)\n",
      "0.000656 * x004**3/x007\n",
      "0.000187 * x003**3/x000\n",
      "-0.000178 * x004**2*x007\n",
      "-0.000121 * sqrt(x002)*x004**3\n",
      "-0.000119 * x001**2/x007\n",
      "0.000093 * x001**3/x000\n",
      "0.000081 * x007**3/x000\n",
      "0.000063 * x001**2*log(x007)\n",
      "0.000014 * x001**2*x004\n",
      "[AutoFeatRegression] Final R^2: 0.9284\n",
      "[AutoFeatRegression] Final dataframe with 89 feature columns (81 new).\n",
      "[AutoFeatRegression] Computing 81 new features.\n",
      "[AutoFeatRegression]    81/   81 new features ...done.\n",
      "autofeat new features: 81\n",
      "autofeat MSE on training data: 20.40077009608341\n",
      "autofeat MSE on test data: 29.013208086449872\n",
      "autofeat R^2 on training data: 0.9284306556376452\n",
      "autofeat R^2 on test data: 0.8856672737185114\n",
      "best params: {'alpha': 25.0}\n",
      "best score: -32.17665941433564\n",
      "MSE on training data: 21.826325386390547\n",
      "MSE on test data: 32.604977274232155\n",
      "R^2 on training data: 0.9234295671003475\n",
      "R^2 on test data: 0.8715131421867837\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 4115 feature combinations from 1035 original feature tuples - done.\n",
      "[feateng] Generated a total of 1048 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 7 new features selected.\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "0.5822755186269846\n",
      "0.123536 * sqrt(x002)/x004\n",
      "-0.111061 * sqrt(x005)*sqrt(x007)\n",
      "0.073677 * x006**2/x004\n",
      "-0.033960 * x004**2/x002\n",
      "-0.009475 * x003\n",
      "0.004178 * exp(x006)/x002\n",
      "0.000208 * x001\n",
      "0.000150 * x002\n",
      "0.000076 * x001**2/x005\n",
      "[AutoFeatRegression] Final R^2: 0.0685\n",
      "[AutoFeatRegression] Final dataframe with 15 feature columns (7 new).\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "autofeat new features: 7\n",
      "autofeat MSE on training data: 1.7392334299147088\n",
      "autofeat MSE on test data: 2.3546899593736006\n",
      "autofeat R^2 on training data: 0.06849813439613206\n",
      "autofeat R^2 on test data: -0.05168395680084359\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -1.9209826492228808\n",
      "MSE on training data: 1.7860752795805885\n",
      "MSE on test data: 2.296009220949717\n",
      "R^2 on training data: 0.04341048968920924\n",
      "R^2 on test data: -0.025475159787906154\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 3570 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng] Generated 11059 feature combinations from 2775 original feature tuples - done.\n",
      "[feateng] Generated a total of 2797 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 24 new features selected.\n",
      "[AutoFeatRegression] Computing 24 new features.\n",
      "[AutoFeatRegression]    24/   24 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-5779.468422222276\n",
      "2116.818547 * exp(x007)/x007\n",
      "30.853871 * x007\n",
      "-8.007633 * x004**2*log(x009)\n",
      "7.103355 * x009**2/x006\n",
      "-3.747951 * x008**3*log(x007)\n",
      "-3.414562 * exp(x001)/x000\n",
      "3.200993 * log(x009)/x005\n",
      "0.734957 * x001\n",
      "-0.719951 * log(x001)/x006\n",
      "0.589573 * x001**2*log(x009)\n",
      "0.513849 * exp(x011)/x005\n",
      "0.367517 * log(x009)/x003\n",
      "-0.290091 * x002**3*log(x003)\n",
      "0.233455 * x011\n",
      "-0.137500 * sqrt(x004)*log(x006)\n",
      "0.134795 * x002\n",
      "-0.108864 * x008\n",
      "0.044319 * x009/x001\n",
      "-0.027350 * x010\n",
      "0.026805 * sqrt(x003)/x001\n",
      "0.018582 * x003\n",
      "-0.010759 * x006/x005\n",
      "-0.007959 * x004*x006\n",
      "0.000531 * x006\n",
      "0.000193 * x003**2/x001\n",
      "0.000180 * sqrt(x002)*x010**3\n",
      "0.000145 * x010**3*log(x005)\n",
      "-0.000140 * x001*x005**2\n",
      "-0.000128 * x005\n",
      "[AutoFeatRegression] Final R^2: 0.3563\n",
      "[AutoFeatRegression] Final dataframe with 36 feature columns (24 new).\n",
      "[AutoFeatRegression] Computing 24 new features.\n",
      "[AutoFeatRegression]    24/   24 new features ...done.\n",
      "autofeat new features: 24\n",
      "autofeat MSE on training data: 0.4866264206121964\n",
      "autofeat MSE on test data: 0.49825543637954367\n",
      "autofeat R^2 on training data: 0.35626937707589446\n",
      "autofeat R^2 on test data: 0.36740163212273413\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -0.49965293677630135\n",
      "MSE on training data: 0.48781914682416255\n",
      "MSE on test data: 0.5005469132113525\n",
      "R^2 on training data: 0.3546915868966416\n",
      "R^2 on test data: 0.3644923121274243\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 630 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 509 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Generated a total of 333 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 66 new features selected.\n",
      "[AutoFeatRegression] Computing 65 new features.\n",
      "[AutoFeatRegression]    65/   65 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "77.90737878175736\n",
      "-16566671.008356 * x004**3/x000\n",
      "12229216.729981 * x004**5\n",
      "9853287.033533 * x002**3*x004**3\n",
      "-66757.618911 * x002**2/x000\n",
      "-29444.644969 * x002**2*x004\n",
      "-26345.216818 * 1/(x000*x003)\n",
      "21365.877720 * sqrt(x004)/x000\n",
      "19067.617781 * x002/x000\n",
      "-15228.755742 * x004**3/x002\n",
      "8562.406279 * x002**3/x003\n",
      "618.531211 * log(x002)/x000\n",
      "-355.192836 * 1/sqrt(x000)\n",
      "202.788598 * 1/x003\n",
      "194.769212 * x002**4\n",
      "-184.814673 * x004*log(x002)\n",
      "149.629809 * log(x002)/x003\n",
      "-108.142793 * sqrt(x002)*log(x002)\n",
      "-106.923987 * x004\n",
      "38.651985 * x003**2*x004**3\n",
      "32.417544 * sqrt(x001)*x002**2\n",
      "27.554745 * x001**2*x002**3\n",
      "-27.229702 * x001/x000\n",
      "18.991912 * 1/(x002*x003)\n",
      "-15.232925 * x001/x003\n",
      "12.297292 * x002\n",
      "-6.705182 * sqrt(x000)*x002**3\n",
      "-6.587863 * 1/(x000*x004)\n",
      "5.634784 * sqrt(x003)*sqrt(x004)\n",
      "-4.970755 * sqrt(x000)/x003\n",
      "-3.930224 * sqrt(x000)*sqrt(x004)\n",
      "2.977574 * x002**3/x004\n",
      "-1.465848 * sqrt(x000)*sqrt(x002)\n",
      "-1.264700 * log(x000)*log(x002)\n",
      "0.239531 * x001*log(x002)\n",
      "0.211258 * x000*x004\n",
      "-0.168871 * x002/x004\n",
      "-0.096730 * x003\n",
      "-0.040328 * sqrt(x003)/x002\n",
      "0.030963 * x000*x002\n",
      "-0.024423 * x003**3*x004**2\n",
      "0.024263 * x001**3/x003\n",
      "-0.019058 * sqrt(x000)*x001\n",
      "0.009776 * 1/(x003*x004)\n",
      "0.007700 * sqrt(x000)/x002\n",
      "-0.003635 * x000**2*x004**3\n",
      "-0.001558 * x003**3/x000\n",
      "0.000544 * sqrt(x001)/x004\n",
      "-0.000463 * x000\n",
      "-0.000307 * x001**2*x003\n",
      "-0.000165 * x001**2/x004\n",
      "-0.000151 * 1/(x002*x004)\n",
      "0.000055 * x003**3*sqrt(x004)\n",
      "0.000049 * sqrt(x000)*x001**3\n",
      "-0.000029 * x002**2*x003**3\n",
      "-0.000013 * x000/x002\n",
      "-0.000011 * x000*x001**2\n",
      "0.000010 * x000**2*x004\n",
      "[AutoFeatRegression] Final R^2: 0.8877\n",
      "[AutoFeatRegression] Final dataframe with 71 feature columns (66 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 65 new features.\n",
      "[AutoFeatRegression]    65/   65 new features ...done.\n",
      "autofeat new features: 65\n",
      "autofeat MSE on training data: 5.265761518456467\n",
      "autofeat MSE on test data: 6.182411519319052\n",
      "autofeat R^2 on training data: 0.8876975913765867\n",
      "autofeat R^2 on test data: 0.8769300508904474\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -6.441314941143723\n",
      "MSE on training data: 5.562096105521496\n",
      "MSE on test data: 6.6656133661696195\n",
      "R^2 on training data: 0.8813776910603296\n",
      "R^2 on test data: 0.867311210974063\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 102466 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.17 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11659 feature combinations from 2926 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 49568 transformed features from 11659 original features - done.\n",
      "[feateng] Generated a total of 52513 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 19 new features selected.\n",
      "[AutoFeatRegression] Computing 19 new features.\n",
      "[AutoFeatRegression]    19/   19 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "20.517447731981342\n",
      "179479794.042325 * 1/(x009**2*x010**3)\n",
      "77.597648 * 1/(x007 + x012)\n",
      "3.689167 * 1/(x002*log(x007))\n",
      "-3.422280 * x004**6*log(x000)**2\n",
      "-3.126635 * 1/(-sqrt(x012) + 1/x007)\n",
      "2.930219 * x008*exp(-sqrt(x012))\n",
      "1.384299 * x005**6/x009**2\n",
      "-0.571854 * log(x000**2 + x010**2)\n",
      "-0.441204 * log(x009**3/x008)\n",
      "-0.357080 * Abs(sqrt(x003) - log(x012))\n",
      "0.304414 * x005**4/x010**2\n",
      "0.214620 * x011*exp(-sqrt(x010))\n",
      "0.121287 * 1/(x004**3*log(x007))\n",
      "-0.088123 * 1/(log(x007) - 1/x005)\n",
      "0.002006 * exp(-x000 + x005)\n",
      "0.000051 * x000**6*x003\n",
      "[AutoFeatRegression] Final R^2: 0.9107\n",
      "[AutoFeatRegression] Final dataframe with 32 feature columns (19 new).\n",
      "[AutoFeatRegression] Computing 19 new features.\n",
      "[AutoFeatRegression]    19/   19 new features ...done.\n",
      "autofeat new features: 19\n",
      "autofeat MSE on training data: 7.594908517930707\n",
      "autofeat MSE on test data: 16.053408200698733\n",
      "autofeat R^2 on training data: 0.9106537410885956\n",
      "autofeat R^2 on test data: 0.8035326148758162\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -12.040607834792462\n",
      "MSE on training data: 6.8650814872458295\n",
      "MSE on test data: 19.788130539098294\n",
      "R^2 on training data: 0.9192394027447124\n",
      "R^2 on test data: 0.7578257392505952\n",
      "#### diabetes\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 60445 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.09 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 7041 feature combinations from 1770 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 31880 transformed features from 7041 original features - done.\n",
      "[feateng] Generated a total of 33661 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 15 new features selected.\n",
      "[AutoFeatRegression] Computing 15 new features.\n",
      "[AutoFeatRegression]    15/   15 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-2353.807920663082\n",
      "2291.410855 * exp(x000*x001)\n",
      "776.359589 * x000**3/x001\n",
      "-249.469313 * Abs(x008 - Abs(x001))\n",
      "173.579585 * Abs(x008 + Abs(x009))\n",
      "167.084461 * x000**9/x009**3\n",
      "163.918946 * 1/(x006 + exp(x001))\n",
      "132.299003 * Abs(x002 + Abs(x009))\n",
      "116.458375 * exp(3*x002)*exp(3*x003)\n",
      "-94.962010 * 1/(exp(x008) + Abs(x000))\n",
      "-27.233359 * x005\n",
      "15.585623 * (-x006 + exp(x002))**3\n",
      "5.817807 * x008/Abs(x008)\n",
      "4.703265 * x000\n",
      "2.400273 * exp(3*x002)*exp(3*x009)\n",
      "1.114631 * x008/Abs(x002)\n",
      "0.035719 * 1/(x002 + x003)\n",
      "[AutoFeatRegression] Final R^2: 0.6250\n",
      "[AutoFeatRegression] Final dataframe with 25 feature columns (15 new).\n",
      "[AutoFeatRegression] Computing 15 new features.\n",
      "[AutoFeatRegression]    15/   15 new features ...done.\n",
      "autofeat new features: 15\n",
      "autofeat MSE on training data: 2303.2392323299123\n",
      "autofeat MSE on test data: 39998.78950615929\n",
      "autofeat R^2 on training data: 0.625048227930895\n",
      "autofeat R^2 on test data: -6.910829432229445\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2683.329471879474\n",
      "MSE on training data: 2376.2751088868854\n",
      "MSE on test data: 72584.07910761613\n",
      "R^2 on training data: 0.6131584811103029\n",
      "R^2 on test data: -13.355441162222688\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.13 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3422 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 11487 transformed features from 3422 original features - done.\n",
      "[feateng] Generated a total of 12360 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 13 new features selected.\n",
      "[AutoFeatRegression] Computing 13 new features.\n",
      "[AutoFeatRegression]    13/   13 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-182.76067257192304\n",
      "-266121.664427 * 1/(x000**2 + x001**3)\n",
      "706.463681 * 1/(log(x003) + 1/x007)\n",
      "-2.101317 * sqrt(x004**2 + x007)\n",
      "1.723690 * x004\n",
      "1.296007 * 1/(-sqrt(x002) + log(x003))\n",
      "-0.728458 * 1/(-sqrt(x007) + log(x005))\n",
      "-0.406805 * x004**(3/2)/x007**3\n",
      "-0.219221 * (sqrt(x003) - sqrt(x007))**2\n",
      "0.146656 * x003\n",
      "0.089929 * x000\n",
      "0.085301 * x007\n",
      "-0.083770 * Abs(x004 - log(x005))\n",
      "0.054391 * x002\n",
      "0.052638 * x001\n",
      "0.029004 * x006\n",
      "0.024746 * x005\n",
      "-0.006016 * Abs(x003 - x004**2)\n",
      "0.000505 * sqrt(sqrt(x002)*x007**3)\n",
      "[AutoFeatRegression] Final R^2: 0.8602\n",
      "[AutoFeatRegression] Final dataframe with 21 feature columns (13 new).\n",
      "[AutoFeatRegression] Computing 13 new features.\n",
      "[AutoFeatRegression]    13/   13 new features ...done.\n",
      "autofeat new features: 13\n",
      "autofeat MSE on training data: 39.859861459734425\n",
      "autofeat MSE on test data: 44.768366037592536\n",
      "autofeat R^2 on training data: 0.8601648791878124\n",
      "autofeat R^2 on test data: 0.8235807179614838\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -44.16700335720902\n",
      "MSE on training data: 41.00523480578115\n",
      "MSE on test data: 47.21993052100021\n",
      "R^2 on training data: 0.8561467161949154\n",
      "R^2 on test data: 0.8139198059310843\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.06 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 4115 feature combinations from 1035 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 16839 transformed features from 4115 original features - done.\n",
      "[feateng] Generated a total of 17887 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 11 new features selected.\n",
      "[AutoFeatRegression] Computing 11 new features.\n",
      "[AutoFeatRegression]    11/   11 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-0.6530628671950001\n",
      "70.598992 * log(x006)**3/x004**3\n",
      "-2.906001 * 1/(sqrt(x002) - x004**2)\n",
      "2.495584 * 1/(-x003**3 + x005)\n",
      "-2.010478 * 1/(-x003**3 + x004**2)\n",
      "-0.097555 * log(x007 + 1/x001)\n",
      "-0.080167 * log(sqrt(x007) + 1/x002)\n",
      "0.042592 * 1/(-sqrt(x004) + x006**2)\n",
      "-0.027371 * x006**4/x004**2\n",
      "0.025026 * x006\n",
      "-0.014029 * x003\n",
      "0.006226 * x000\n",
      "0.005774 * 1/(sqrt(x005) - log(x002))\n",
      "0.003693 * x004\n",
      "0.000615 * x001\n",
      "0.000120 * x002\n",
      "-0.000109 * x005\n",
      "0.000104 * x006**9/x002**3\n",
      "[AutoFeatRegression] Final R^2: 0.1154\n",
      "[AutoFeatRegression] Final dataframe with 19 feature columns (11 new).\n",
      "[AutoFeatRegression] Computing 11 new features.\n",
      "[AutoFeatRegression]    11/   11 new features ...done.\n",
      "autofeat new features: 11\n",
      "autofeat MSE on training data: 1.6515936416000805\n",
      "autofeat MSE on test data: 6.739887090828429\n",
      "autofeat R^2 on training data: 0.11543641473967836\n",
      "autofeat R^2 on test data: -2.010260903290638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'alpha': 1e-05}\n",
      "best score: -1.9787928981159926\n",
      "MSE on training data: 1.6131791121768742\n",
      "MSE on test data: 3.2417999332340495\n",
      "R^2 on training data: 0.13601053964352539\n",
      "R^2 on test data: -0.4478971923111519\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 87234 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 1.81 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11059 feature combinations from 2775 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 50887 transformed features from 11059 original features - done.\n",
      "[feateng] Generated a total of 53684 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 18 new features selected.\n",
      "[AutoFeatRegression] Computing 18 new features.\n",
      "[AutoFeatRegression]    18/   18 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "41.91631190035072\n",
      "-202.946886 * 1/(x003**3 + x010**3)\n",
      "-35.249049 * x007\n",
      "-16.651286 * 1/(x010 + 1/x001)\n",
      "10.736055 * x009**2/x006\n",
      "-1.755968 * Abs(log(x009))/Abs(x005)\n",
      "-0.421278 * x001**2*log(x009)**2\n",
      "-0.243995 * (-exp(x011) + log(x000))**2\n",
      "-0.207425 * x009\n",
      "-0.198819 * log(x009)**2/x003**2\n",
      "-0.190975 * x001\n",
      "-0.126835 * x002\n",
      "-0.119953 * Abs(log(x004) + log(x005))\n",
      "0.105068 * x008\n",
      "0.068841 * Abs(log(x004) + 1/x009)\n",
      "-0.033525 * 1/(x002 + x009**3)\n",
      "-0.028653 * (log(x005) - log(x006))**2\n",
      "0.027630 * x000\n",
      "0.002144 * x011\n",
      "0.000121 * (x010 + log(x003))**3\n",
      "[AutoFeatRegression] Final R^2: 0.3466\n",
      "[AutoFeatRegression] Final dataframe with 30 feature columns (18 new).\n",
      "[AutoFeatRegression] Computing 18 new features.\n",
      "[AutoFeatRegression]    18/   18 new features ...done.\n",
      "autofeat new features: 18\n",
      "autofeat MSE on training data: 0.4939064274422446\n",
      "autofeat MSE on test data: 0.505050190695415\n",
      "autofeat R^2 on training data: 0.3466390669794902\n",
      "autofeat R^2 on test data: 0.35877483113571385\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -0.5015823941642708\n",
      "MSE on training data: 0.49151805968872464\n",
      "MSE on test data: 0.5007439817101309\n",
      "R^2 on training data: 0.349798503862944\n",
      "R^2 on test data: 0.364242108714506\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 14910 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 509 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1906 transformed features from 509 original features - done.\n",
      "[feateng] Generated a total of 2239 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 58 new features selected.\n",
      "[AutoFeatRegression] Computing 58 new features.\n",
      "[AutoFeatRegression]    58/   58 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "145.3075844889296\n",
      "-7909785501854843904.000000 * x004**9/x000**3\n",
      "-1493537874903014400.000000 * x004**15\n",
      "-693133149535.494141 * x002**9/x000**3\n",
      "-1675830422.233405 * x004**3/x003**3\n",
      "19989140.029255 * x002*exp(-sqrt(x000))\n",
      "-993602.986295 * x004/x003**3\n",
      "-794210.466369 * log(x002)**3/x000**3\n",
      "-1818.710732 * sqrt(x002**3)*Abs(x004)\n",
      "-908.481033 * 1/(x000**2*x004)\n",
      "202.727407 * x004\n",
      "-120.380444 * 1/(x000*x002)\n",
      "-107.516493 * log(x000)**3/x003**3\n",
      "-79.633327 * sqrt(x002)/x003\n",
      "17.251493 * sqrt(x001)/x000\n",
      "-11.036892 * sqrt(sqrt(x000)*x004)\n",
      "-9.258938 * 1/(x000*x004)\n",
      "9.209722 * sqrt(x001)*x002**3\n",
      "5.157487 * x001**6*x002**9\n",
      "2.285851 * 1/(x003**2*x004)\n",
      "1.295090 * 1/(x002**3*x003**3)\n",
      "0.581242 * 1/(-sqrt(x000) + 1/x002)\n",
      "-0.184889 * x001**3/x000\n",
      "-0.045606 * (log(x000) + log(x002))**3\n",
      "-0.032982 * Abs(sqrt(x000) - 1/x002)\n",
      "-0.010819 * sqrt(x000)*sqrt(x001)\n",
      "-0.009316 * x001**4*x004**2\n",
      "-0.005390 * x001*log(x000)**2\n",
      "0.000845 * 1/(x002**2 + 1/x000)\n",
      "-0.000494 * x002**9/x004**3\n",
      "-0.000386 * x001**6*x002**4\n",
      "0.000084 * x000**(3/2)*x002**3\n",
      "-0.000013 * 1/(x002**2 - 1/x000)\n",
      "[AutoFeatRegression] Final R^2: 0.8600\n",
      "[AutoFeatRegression] Final dataframe with 64 feature columns (59 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 58 new features.\n",
      "[AutoFeatRegression]    58/   58 new features ...done.\n",
      "autofeat new features: 58\n",
      "autofeat MSE on training data: 6.564765030406414\n",
      "autofeat MSE on test data: 7.2970392613535155\n",
      "autofeat R^2 on training data: 0.8599938636838419\n",
      "autofeat R^2 on test data: 0.8547417544531074\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -6.768951378875196\n",
      "MSE on training data: 5.9947784597159615\n",
      "MSE on test data: 6.908709166222112\n",
      "R^2 on training data: 0.8721499145317924\n",
      "R^2 on test data: 0.8624720333088886\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
