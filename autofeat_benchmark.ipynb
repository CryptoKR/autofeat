{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import FeatureSelector, AutoFeatRegression\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"diabetes\", \"boston\", \"concrete\", \"airfoil\", \"wine_quality\", \"forest_fires\"]\n",
    "\n",
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "\n",
    "    elif name == \"concrete\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "        # Cement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Blast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Water (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Superplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Coarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fine Aggregate (component 7)    -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Age -- quantitative -- Day (1~365) -- Input Variable\n",
    "        # Concrete compressive strength -- quantitative -- MPa -- Output Variable\n",
    "        df = pd.read_csv(os.path.join(datapath, \"concrete.csv\"))\n",
    "        X = df.iloc[:, :8].to_numpy()\n",
    "        y = df.iloc[:, 8].to_numpy()\n",
    "\n",
    "    elif name == \"forest_fires\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        # 1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "        # 2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "        # 3. month - month of the year: 'jan' to 'dec'\n",
    "        # 4. day - day of the week: 'mon' to 'sun'\n",
    "        # 5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "        # 6. DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "        # 7. DC - DC index from the FWI system: 7.9 to 860.6\n",
    "        # 8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "        # 9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "        # 10. RH - relative humidity in %: 15.0 to 100\n",
    "        # 11. wind - wind speed in km/h: 0.40 to 9.40\n",
    "        # 12. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "        # 13. area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "        # (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
    "        # --> first 4 are ignored\n",
    "        df = pd.read_csv(os.path.join(datapath, \"forest_fires.csv\"))\n",
    "        X = df.iloc[:, 4:12].to_numpy()\n",
    "        y = df.iloc[:, 12].to_numpy()\n",
    "        # perform transformation as they suggested\n",
    "        y = np.log(y + 1)\n",
    "\n",
    "    elif name == \"wine_quality\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "        # Input variables (based on physicochemical tests):\n",
    "        # 1 - fixed acidity\n",
    "        # 2 - volatile acidity\n",
    "        # 3 - citric acid\n",
    "        # 4 - residual sugar\n",
    "        # 5 - chlorides\n",
    "        # 6 - free sulfur dioxide\n",
    "        # 7 - total sulfur dioxide\n",
    "        # 8 - density\n",
    "        # 9 - pH\n",
    "        # 10 - sulphates\n",
    "        # 11 - alcohol\n",
    "        # Output variable (based on sensory data):\n",
    "        # 12 - quality (score between 0 and 10)\n",
    "        df_red = pd.read_csv(os.path.join(datapath, \"winequality-red.csv\"), sep=\";\")\n",
    "        df_white = pd.read_csv(os.path.join(datapath, \"winequality-white.csv\"), sep=\";\")\n",
    "        # add additional categorical feature for red or white\n",
    "        X = np.hstack([np.vstack([df_red.iloc[:, :-1].to_numpy(), df_white.iloc[:, :-1].to_numpy()]), np.array([[1]*len(df_red) + [0]*len(df_white)]).T])\n",
    "        y = np.hstack([df_red[\"quality\"].to_numpy(), df_white[\"quality\"].to_numpy()])\n",
    "\n",
    "    elif name == \"airfoil\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise\n",
    "        # This problem has the following inputs:\n",
    "        # 1. Frequency, in Hertz.\n",
    "        # 2. Angle of attack, in degrees.\n",
    "        # 3. Chord length, in meters.\n",
    "        # 4. Free-stream velocity, in meters per second.\n",
    "        # 5. Suction side displacement thickness, in meters.\n",
    "        # The only output is:\n",
    "        # 6. Scaled sound pressure level, in decibels.\n",
    "        units = {\"x001\": \"Hz\", \"x003\": \"m\", \"x004\": \"m/sec\", \"x005\": \"m\"}\n",
    "        df = pd.read_csv(os.path.join(datapath, \"airfoil_self_noise.tsv\"), header=None, names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"y\"], sep=\"\\t\")\n",
    "        X = df.iloc[:, :5].to_numpy()\n",
    "        y = df[\"y\"].to_numpy()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units\n",
    "\n",
    "def test_model(dataset, model, param_grid):\n",
    "    # load data\n",
    "    X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, iid=False)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(dataset, feateng_steps=2):\n",
    "    # load data\n",
    "    X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegression(verbose=1, feateng_steps=feateng_steps, units=units, featsel_w_thr=0.01)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5, iid=False)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5, iid=False)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "(442, 10)\n",
      "#### boston\n",
      "(506, 13)\n",
      "#### concrete\n",
      "(1030, 8)\n",
      "#### airfoil\n",
      "(1503, 5)\n",
      "#### wine_quality\n",
      "(6497, 12)\n",
      "#### forest_fires\n",
      "(517, 8)\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    X, y, _ = load_regression_dataset(dsname)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3043.14487668777\n",
      "MSE on training data: 2817.5756461735427\n",
      "MSE on test data: 3119.632550355442\n",
      "R^2 on training data: 0.541317737800587\n",
      "R^2 on test data: 0.38300930348673157\n",
      "#### boston\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -25.427148426837693\n",
      "MSE on training data: 22.4278718761592\n",
      "MSE on test data: 20.5580503052922\n",
      "R^2 on training data: 0.7361592384229154\n",
      "R^2 on test data: 0.7484031841564716\n",
      "#### concrete\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -110.34480414200297\n",
      "MSE on training data: 107.00865107837934\n",
      "MSE on test data: 110.56229503996865\n",
      "R^2 on training data: 0.6245955930727385\n",
      "R^2 on test data: 0.5643057266127824\n",
      "#### airfoil\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -22.960476312553066\n",
      "MSE on training data: 22.6317043193984\n",
      "MSE on test data: 24.732769352718233\n",
      "R^2 on training data: 0.5173357362628234\n",
      "R^2 on test data: 0.5076580301932743\n",
      "#### wine_quality\n",
      "best params: {'alpha': 0.0001}\n",
      "best score: -0.5401265191014872\n",
      "MSE on training data: 0.5348196387905402\n",
      "MSE on test data: 0.5434554548609953\n",
      "R^2 on training data: 0.29251728914027897\n",
      "R^2 on test data: 0.3100144852264428\n",
      "#### forest_fires\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.8969026573257473\n",
      "MSE on training data: 1.8469604643703812\n",
      "MSE on test data: 2.328893858372984\n",
      "R^2 on training data: 0.010801489514856932\n",
      "R^2 on test data: -0.0401625480215837\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rreg = Ridge()\n",
    "    params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    rreg = test_model(dsname, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'C': 10.0}\n",
      "best score: -3057.7070015538065\n",
      "MSE on training data: 2577.0841482085507\n",
      "MSE on test data: 3437.6800004513143\n",
      "R^2 on training data: 0.5804681274187382\n",
      "R^2 on test data: 0.32010692168648935\n",
      "#### boston\n",
      "best params: {'C': 100.0}\n",
      "best score: -13.598043246310898\n",
      "MSE on training data: 3.4469342608444284\n",
      "MSE on test data: 9.636188588435925\n",
      "R^2 on training data: 0.9594503764998732\n",
      "R^2 on test data: 0.8820688572255264\n",
      "#### concrete\n",
      "best params: {'C': 100.0}\n",
      "best score: -37.08377959823637\n",
      "MSE on training data: 18.997096173790347\n",
      "MSE on test data: 30.152373071635388\n",
      "R^2 on training data: 0.9333549806432162\n",
      "R^2 on test data: 0.8811781514521082\n",
      "#### airfoil\n",
      "best params: {'C': 250.0}\n",
      "best score: -7.094189398840056\n",
      "MSE on training data: 5.457762290823167\n",
      "MSE on test data: 7.477589784074904\n",
      "R^2 on training data: 0.8836028086716045\n",
      "R^2 on test data: 0.8511476320667879\n",
      "#### wine_quality\n",
      "best params: {'C': 10.0}\n",
      "best score: -0.4640006945371349\n",
      "MSE on training data: 0.32390678929749517\n",
      "MSE on test data: 0.4638085434435221\n",
      "R^2 on training data: 0.5715219922060317\n",
      "R^2 on test data: 0.4111363245289218\n",
      "#### forest_fires\n",
      "best params: {'C': 1.0}\n",
      "best score: -2.186927498252301\n",
      "MSE on training data: 1.8539927476295148\n",
      "MSE on test data: 3.12614582192515\n",
      "R^2 on training data: 0.007035126206362374\n",
      "R^2 on test data: -0.396242165322382\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    svr = test_model(dsname, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3336.6571277553485\n",
      "MSE on training data: 2472.319475907154\n",
      "MSE on test data: 3268.607555103584\n",
      "R^2 on training data: 0.5975231076301993\n",
      "R^2 on test data: 0.35354551553768243\n",
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.462810946604932\n",
      "MSE on training data: 1.4186988960396048\n",
      "MSE on test data: 10.583239343137262\n",
      "R^2 on training data: 0.9833104719321342\n",
      "R^2 on test data: 0.8704785093673091\n",
      "#### concrete\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.701648337050347\n",
      "MSE on training data: 4.169688233206215\n",
      "MSE on test data: 27.527437198114896\n",
      "R^2 on training data: 0.9853720299949222\n",
      "R^2 on test data: 0.8915222703733743\n",
      "#### airfoil\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.770457737476125\n",
      "MSE on training data: 0.4389576916890236\n",
      "MSE on test data: 3.316904702700349\n",
      "R^2 on training data: 0.9906383899294207\n",
      "R^2 on test data: 0.9339721576787678\n",
      "#### wine_quality\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3930715087362109\n",
      "MSE on training data: 0.05242462959399654\n",
      "MSE on test data: 0.3478176153846154\n",
      "R^2 on training data: 0.9306504167557255\n",
      "R^2 on test data: 0.558401495004132\n",
      "#### forest_fires\n",
      "best params: {'min_samples_leaf': 0.2}\n",
      "best score: -1.8836034096444383\n",
      "MSE on training data: 1.828520757317779\n",
      "MSE on test data: 2.2946977602510548\n",
      "R^2 on training data: 0.020677462012424486\n",
      "R^2 on test data: -0.024889417205844477\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    rforest = test_model(dsname, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 70 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Generated a total of 50 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 39/60 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 20 features after 3 feature selection runs\n",
      "[featsel] 16 features after correlation filtering\n",
      "[featsel] 8 features after noise filtering\n",
      "[featsel] 13 final features selected (including 10 original keep features).\n",
      "[AutoFeatRegression] Computing 3 new features.\n",
      "[AutoFeatRegression]     3/    3 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "143.5564466101628\n",
      "12982.173074 * x003**3\n",
      "2366.679823 * x009**2\n",
      "1585.770405 * x002**2\n",
      "700.932005 * x008\n",
      "-603.019521 * x004\n",
      "468.853861 * x002\n",
      "385.380742 * x005\n",
      "274.786506 * x003\n",
      "-206.371250 * x001\n",
      "98.815335 * x007\n",
      "75.635364 * x009\n",
      "-17.982955 * x000\n",
      "[AutoFeatRegression] Final R^2: 0.5564\n",
      "[AutoFeatRegression] Final dataframe with 13 feature columns (3 new).\n",
      "[AutoFeatRegression] Computing 3 new features.\n",
      "[AutoFeatRegression]     3/    3 new features ...done.\n",
      "autofeat new features: 3\n",
      "autofeat MSE on training data: 2724.9429612820513\n",
      "autofeat MSE on test data: 3052.2959351855798\n",
      "autofeat R^2 on training data: 0.5563977125005876\n",
      "autofeat R^2 on test data: 0.39632691843781465\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -3006.339803040455\n",
      "MSE on training data: 2732.2049241116715\n",
      "MSE on test data: 3050.434148071183\n",
      "R^2 on training data: 0.5552155140587385\n",
      "R^2 on test data: 0.39669513658848765\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3343.388820276542\n",
      "MSE on training data: 2456.8920537762856\n",
      "MSE on test data: 3268.3291640536463\n",
      "R^2 on training data: 0.6000345876298587\n",
      "R^2 on test data: 0.35360057480669316\n",
      "#### boston\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 91 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Generated a total of 64 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 76/77 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 30 features after 3 feature selection runs\n",
      "[featsel] 17 features after correlation filtering\n",
      "[featsel] 14 features after noise filtering\n",
      "[featsel] 17 final features selected (including 13 original keep features).\n",
      "[AutoFeatRegression] Computing 4 new features.\n",
      "[AutoFeatRegression]     4/    4 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "31.604816406630537\n",
      "39.558545 * 1/x012\n",
      "-18.205236 * x004\n",
      "17.651258 * 1/x007\n",
      "-1.891980 * 1/x008\n",
      "1.594141 * x003\n",
      "0.861523 * x005\n",
      "-0.540312 * x010\n",
      "-0.356491 * x012\n",
      "0.170787 * x008\n",
      "-0.157209 * x000\n",
      "-0.120094 * x007\n",
      "-0.058971 * x002\n",
      "-0.009475 * x009\n",
      "0.004720 * x011\n",
      "0.003473 * x006\n",
      "0.002409 * exp(x005)\n",
      "[AutoFeatRegression] Final R^2: 0.8294\n",
      "[AutoFeatRegression] Final dataframe with 17 feature columns (4 new).\n",
      "[AutoFeatRegression] Computing 4 new features.\n",
      "[AutoFeatRegression]     4/    4 new features ...done.\n",
      "autofeat new features: 4\n",
      "autofeat MSE on training data: 14.501955635374268\n",
      "autofeat MSE on test data: 16.154268586857356\n",
      "autofeat R^2 on training data: 0.82939946151282\n",
      "autofeat R^2 on test data: 0.8022982491832813\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -17.075190018192423\n",
      "MSE on training data: 14.17437283137346\n",
      "MSE on test data: 16.903276867520784\n",
      "R^2 on training data: 0.8332531350563631\n",
      "R^2 on test data: 0.793131616372443\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.321511567376538\n",
      "MSE on training data: 1.422155388613863\n",
      "MSE on test data: 10.821445500000001\n",
      "R^2 on training data: 0.9832698098649432\n",
      "R^2 on test data: 0.8675632567197582\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Generated a total of 34 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 41/42 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 34 features after 3 feature selection runs\n",
      "[featsel] 10 features after correlation filtering\n",
      "[featsel] 10 features after noise filtering\n",
      "[featsel] 10 final features selected (including 8 original keep features).\n",
      "[AutoFeatRegression] Computing 2 new features.\n",
      "[AutoFeatRegression]     2/    2 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-87.53868804697464\n",
      "-69.200779 * 1/x007\n",
      "0.530259 * x004\n",
      "0.145704 * x000\n",
      "0.123821 * x001\n",
      "0.092890 * x002\n",
      "-0.065418 * x003\n",
      "0.064258 * x007\n",
      "0.051124 * x006\n",
      "0.042308 * x005\n",
      "-0.000832 * x004**3\n",
      "[AutoFeatRegression] Final R^2: 0.8000\n",
      "[AutoFeatRegression] Final dataframe with 10 feature columns (2 new).\n",
      "[AutoFeatRegression] Computing 2 new features.\n",
      "[AutoFeatRegression]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat MSE on training data: 57.02094988174077\n",
      "autofeat MSE on test data: 68.1056430536676\n",
      "autofeat R^2 on training data: 0.7999608848717749\n",
      "autofeat R^2 on test data: 0.7316152070368118\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -59.17985371133236\n",
      "MSE on training data: 57.02102364761072\n",
      "MSE on test data: 68.08141806221839\n",
      "R^2 on training data: 0.7999606260886543\n",
      "R^2 on test data: 0.7317106707755436\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.521670484294408\n",
      "MSE on training data: 4.148196057261051\n",
      "MSE on test data: 27.854001934481715\n",
      "R^2 on training data: 0.9854474281751906\n",
      "R^2 on test data: 0.8902353724713925\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 35 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Generated a total of 22 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 27/27 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 18 features after 3 feature selection runs\n",
      "[featsel] 9 features after correlation filtering\n",
      "[featsel] 9 features after noise filtering\n",
      "[featsel] 9 final features selected (including 6 original keep features).\n",
      "[AutoFeatRegression] Computing 3 new features.\n",
      "[AutoFeatRegression]     3/    3 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "128.38440052554438\n",
      "-46075.161280 * x004**3\n",
      "407.375185 * 1/x000\n",
      "-29.008010 * x004\n",
      "-24.158945 * x002\n",
      "-0.298405 * x001\n",
      "0.088590 * x003\n",
      "0.003397 * 1/x004\n",
      "-0.001290 * x000\n",
      "[AutoFeatRegression] Final R^2: 0.5440\n",
      "[AutoFeatRegression] Final dataframe with 9 feature columns (4 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 3 new features.\n",
      "[AutoFeatRegression]     3/    3 new features ...done.\n",
      "autofeat new features: 3\n",
      "autofeat MSE on training data: 21.37937690632624\n",
      "autofeat MSE on test data: 23.533548550656427\n",
      "autofeat R^2 on training data: 0.5440440071140936\n",
      "autofeat R^2 on test data: 0.5315302752904623\n",
      "# Ridge Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'alpha': 1e-05}\n",
      "best score: -22.31511878239506\n",
      "MSE on training data: 21.65038669763258\n",
      "MSE on test data: 24.040480113582436\n",
      "R^2 on training data: 0.5382642063734873\n",
      "R^2 on test data: 0.5214390606477008\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.8499058438152183\n",
      "MSE on training data: 0.44608849616581564\n",
      "MSE on test data: 3.4021282771618115\n",
      "R^2 on training data: 0.9904863119222114\n",
      "R^2 on test data: 0.9322756577063652\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 84 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Generated a total of 63 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 60/75 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 29 features after 3 feature selection runs\n",
      "[featsel] 18 features after correlation filtering\n",
      "[featsel] 7 features after noise filtering\n",
      "[featsel] 12 final features selected (including 12 original keep features).\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "109.14656748734144\n",
      "-108.667704 * x007\n",
      "-1.516594 * x001\n",
      "0.702682 * x009\n",
      "-0.684531 * x004\n",
      "0.534453 * x008\n",
      "0.388732 * x011\n",
      "0.210429 * x010\n",
      "0.082823 * x000\n",
      "0.064116 * x003\n",
      "-0.039908 * x002\n",
      "0.004649 * x005\n",
      "-0.001295 * x006\n",
      "[AutoFeatRegression] Final R^2: 0.2925\n",
      "[AutoFeatRegression] Final dataframe with 12 feature columns (0 new).\n",
      "autofeat new features: 0\n",
      "autofeat MSE on training data: 0.5348100541854347\n",
      "autofeat MSE on test data: 0.5435423543976734\n",
      "autofeat R^2 on training data: 0.29252996807334486\n",
      "autofeat R^2 on test data: 0.3099041552610112\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.0001}\n",
      "best score: -0.5401265191014872\n",
      "MSE on training data: 0.5348196387905402\n",
      "MSE on test data: 0.5434554548609953\n",
      "R^2 on training data: 0.29251728914027897\n",
      "R^2 on test data: 0.3100144852264428\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3930715087362109\n",
      "MSE on training data: 0.05242462959399654\n",
      "MSE on test data: 0.3478176153846154\n",
      "R^2 on training data: 0.9306504167557255\n",
      "R^2 on test data: 0.558401495004132\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Generated a total of 38 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 8/46 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 0 features after noise filtering\n",
      "[featsel] WARNING: Not a single good features was found...\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "1.034362126350683\n",
      "0.000050 * x002\n",
      "[AutoFeatRegression] Final R^2: 0.0017\n",
      "[AutoFeatRegression] Final dataframe with 8 feature columns (0 new).\n",
      "autofeat new features: 0\n",
      "autofeat MSE on training data: 1.863966348455665\n",
      "autofeat MSE on test data: 2.302383639455436\n",
      "autofeat R^2 on training data: 0.0016934465809870236\n",
      "autofeat R^2 on test data: -0.02832219009425807\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.8969026573257473\n",
      "MSE on training data: 1.8469604643703812\n",
      "MSE on test data: 2.328893858372984\n",
      "R^2 on training data: 0.010801489514856932\n",
      "R^2 on test data: -0.0401625480215837\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.2}\n",
      "best score: -1.8836034096444383\n",
      "MSE on training data: 1.828520757317779\n",
      "MSE on test data: 2.2946977602510548\n",
      "R^2 on training data: 0.020677462012424486\n",
      "R^2 on test data: -0.024889417205844477\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 2485 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 7041 feature combinations from 1770 original feature tuples - done.\n",
      "[feateng] Generated a total of 1781 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 918/1791 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 17 features after 3 feature selection runs\n",
      "[featsel] 17 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[featsel] 16 final features selected (including 10 original keep features).\n",
      "[AutoFeatRegression] Computing 6 new features.\n",
      "[AutoFeatRegression]     6/    6 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-240.79235653504017\n",
      "-1657.883467 * x006*Abs(x009)\n",
      "254.634095 * exp(x002)*exp(x008)\n",
      "250.142104 * exp(x002)*exp(x003)\n",
      "-136.095251 * exp(x001)*exp(x006)\n",
      "24.017144 * exp(x003)*exp(x008)\n",
      "9.229191 * Abs(x008)/x008\n",
      "[AutoFeatRegression] Final R^2: 0.5388\n",
      "[AutoFeatRegression] Final dataframe with 16 feature columns (6 new).\n",
      "[AutoFeatRegression] Computing 6 new features.\n",
      "[AutoFeatRegression]     6/    6 new features ...done.\n",
      "autofeat new features: 6\n",
      "autofeat MSE on training data: 2833.26451164592\n",
      "autofeat MSE on test data: 3022.267232650059\n",
      "autofeat R^2 on training data: 0.538763696592861\n",
      "autofeat R^2 on test data: 0.40226589676097324\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2987.860836302082\n",
      "MSE on training data: 2755.8792472252694\n",
      "MSE on test data: 3052.4833233321756\n",
      "R^2 on training data: 0.5513614943462484\n",
      "R^2 on test data: 0.39628985742462797\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3083.691101051751\n",
      "MSE on training data: 2227.27748723074\n",
      "MSE on test data: 3389.853695432779\n",
      "R^2 on training data: 0.6374142863648617\n",
      "R^2 on test data: 0.3295658514702807\n",
      "#### boston\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11659 feature combinations from 2926 original feature tuples - done.\n",
      "[feateng] Generated a total of 2945 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 2706/2958 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 44 features after 3 feature selection runs\n",
      "[featsel] 35 features after correlation filtering\n",
      "[featsel] 15 features after noise filtering\n",
      "[featsel] 26 final features selected (including 13 original keep features).\n",
      "[AutoFeatRegression] Computing 13 new features.\n",
      "[AutoFeatRegression]    13/   13 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "22.54838943833453\n",
      "76.926996 * 1/(x007*x012)\n",
      "15.965998 * 1/(x002*x007)\n",
      "5.808109 * sqrt(x008)/x012\n",
      "-2.629609 * sqrt(x000)*x004**3\n",
      "1.824749 * x005**3/x009\n",
      "0.585995 * exp(x005)/x009\n",
      "-0.371035 * x010\n",
      "-0.124615 * x012\n",
      "0.028017 * x000**3*x003**3\n",
      "-0.000712 * exp(x005)*log(x000)\n",
      "-0.000141 * x005**3*x012\n",
      "0.000022 * x005**3*x011\n",
      "-0.000019 * x009**2/x008\n",
      "[AutoFeatRegression] Final R^2: 0.8861\n",
      "[AutoFeatRegression] Final dataframe with 26 feature columns (13 new).\n",
      "[AutoFeatRegression] Computing 13 new features.\n",
      "[AutoFeatRegression]    13/   13 new features ...done.\n",
      "autofeat new features: 13\n",
      "autofeat MSE on training data: 9.680260055228018\n",
      "autofeat MSE on test data: 14.877133395154106\n",
      "autofeat R^2 on training data: 0.8861217328448117\n",
      "autofeat R^2 on test data: 0.8179282891366096\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -13.964195078656934\n",
      "MSE on training data: 8.425014528662516\n",
      "MSE on test data: 14.91243103667503\n",
      "R^2 on training data: 0.9008884007446457\n",
      "R^2 on test data: 0.8174963039005795\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -11.285766618950612\n",
      "MSE on training data: 1.5165601658415844\n",
      "MSE on test data: 11.004116235294122\n",
      "R^2 on training data: 0.982159235109665\n",
      "R^2 on test data: 0.8653276665414441\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3422 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Generated a total of 873 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 799/881 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 84 features after 3 feature selection runs\n",
      "[featsel] 49 features after correlation filtering\n",
      "[featsel] 33 features after noise filtering\n",
      "[featsel] 38 final features selected (including 8 original keep features).\n",
      "[AutoFeatRegression] Computing 30 new features.\n",
      "[AutoFeatRegression]    30/   30 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-76.33457180104278\n",
      "-26377471.212510 * 1/(x005*x006)\n",
      "-453761.931913 * 1/(x000*x003)\n",
      "2593.517869 * 1/(x000*x007)\n",
      "223.224349 * sqrt(x000)/x003\n",
      "-93.731442 * x004/x000\n",
      "88.280080 * sqrt(x001)/x000\n",
      "-0.589718 * x004/x007\n",
      "0.503441 * x003\n",
      "0.353408 * sqrt(x000)*log(x007)\n",
      "0.120854 * sqrt(x002)*sqrt(x007)\n",
      "0.071583 * x006\n",
      "-0.030567 * x000/x007\n",
      "0.026291 * x001*log(x007)\n",
      "-0.007813 * x005\n",
      "-0.003898 * x007\n",
      "-0.000074 * x001**3/x003\n",
      "[AutoFeatRegression] Final R^2: 0.9031\n",
      "[AutoFeatRegression] Final dataframe with 38 feature columns (30 new).\n",
      "[AutoFeatRegression] Computing 30 new features.\n",
      "[AutoFeatRegression]    30/   30 new features ...done.\n",
      "autofeat new features: 30\n",
      "autofeat MSE on training data: 27.621290496648307\n",
      "autofeat MSE on test data: 35.79934379039252\n",
      "autofeat R^2 on training data: 0.903099851526351\n",
      "autofeat R^2 on test data: 0.8589250605293993\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -32.06120910314486\n",
      "MSE on training data: 28.125706487802532\n",
      "MSE on test data: 38.11817098553751\n",
      "R^2 on training data: 0.9013302751033645\n",
      "R^2 on test data: 0.8497872280564573\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -26.308392171616948\n",
      "MSE on training data: 4.340218399611623\n",
      "MSE on test data: 28.30466308741183\n",
      "R^2 on training data: 0.9847737813922394\n",
      "R^2 on test data: 0.8884594462073914\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 630 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 509 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Generated a total of 333 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 318/338 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 77 features after 3 feature selection runs\n",
      "[featsel] 48 features after correlation filtering\n",
      "[featsel] 45 features after noise filtering\n",
      "[featsel] 45 final features selected (including 6 original keep features).\n",
      "[AutoFeatRegression] Computing 39 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeatRegression]    39/   39 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "158.2080194630043\n",
      "18856460.524305 * x004**5\n",
      "-15382381.199017 * x004**3/x000\n",
      "-150973.482290 * x004**2/x003\n",
      "25919.792241 * sqrt(x004)/x000\n",
      "-20776.979268 * x002**2*x004\n",
      "-13645.343423 * x002**2/x000\n",
      "10221.623300 * x002**3/x003\n",
      "1606.320052 * log(x002)/x000\n",
      "-1129.838461 * sqrt(x004)/x003\n",
      "214.235439 * x004\n",
      "83.982768 * sqrt(x000)*x004**3\n",
      "34.157947 * sqrt(x001)/x003\n",
      "-27.762247 * x002\n",
      "16.290260 * x001**2*x002**3\n",
      "-9.221254 * 1/(x000*x004)\n",
      "-5.651801 * sqrt(x000)/x003\n",
      "-5.034130 * sqrt(x001)*x002**2\n",
      "4.463691 * 1/(x002*x003)\n",
      "-1.112450 * x001\n",
      "-0.755724 * sqrt(x000)*sqrt(x002)\n",
      "0.458290 * sqrt(x000)*sqrt(x004)\n",
      "-0.422113 * x001**3*x004**2\n",
      "-0.333056 * x001**3/x000\n",
      "0.218247 * x002**2/x004\n",
      "0.168858 * 1/(x003*x004)\n",
      "-0.154612 * x002/x004\n",
      "-0.073704 * sqrt(x003)/x002\n",
      "0.068368 * x003\n",
      "0.050428 * x001**3/x003\n",
      "-0.027407 * sqrt(x000)*x001\n",
      "0.014329 * sqrt(x000)/x002\n",
      "0.004426 * x000\n",
      "0.002038 * x000*log(x002)\n",
      "0.000221 * x002**3*x003**3\n",
      "-0.000138 * 1/(x002*x004)\n",
      "-0.000120 * x001**2/x004\n",
      "[AutoFeatRegression] Final R^2: 0.8787\n",
      "[AutoFeatRegression] Final dataframe with 45 feature columns (40 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 39 new features.\n",
      "[AutoFeatRegression]    39/   39 new features ...done.\n",
      "autofeat new features: 39\n",
      "autofeat MSE on training data: 5.689815617875137\n",
      "autofeat MSE on test data: 6.743676252733724\n",
      "autofeat R^2 on training data: 0.8786538288392152\n",
      "autofeat R^2 on test data: 0.8657572549737599\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -7.169986079624747\n",
      "MSE on training data: 6.429391225767707\n",
      "MSE on test data: 7.339514420653454\n",
      "R^2 on training data: 0.8628809682882104\n",
      "R^2 on test data: 0.8538962242458203\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.612228622438109\n",
      "MSE on training data: 0.43041735691073185\n",
      "MSE on test data: 3.180465044722944\n",
      "R^2 on training data: 0.9908205288589356\n",
      "R^2 on test data: 0.9366881887471192\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 3570 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11059 feature combinations from 2775 original feature tuples - done.\n",
      "[feateng] Generated a total of 2797 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 2272/2809 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 60 features after 3 feature selection runs\n",
      "[featsel] 50 features after correlation filtering\n",
      "[featsel] 19 features after noise filtering\n",
      "[featsel] 31 final features selected (including 12 original keep features).\n",
      "[AutoFeatRegression] Computing 19 new features.\n",
      "[AutoFeatRegression]    19/   19 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-4463.089590265891\n",
      "1663.889034 * exp(x007)/x007\n",
      "-56.032417 * x007\n",
      "7.105059 * x009**2/x006\n",
      "-6.544285 * x004**2*log(x009)\n",
      "3.168149 * log(x009)/x005\n",
      "-2.604628 * sqrt(x001)/x010\n",
      "1.423576 * x011/x005\n",
      "-0.384825 * x004\n",
      "0.351994 * log(x009)/x003\n",
      "0.330605 * x001*log(x009)\n",
      "-0.258989 * x001\n",
      "0.233178 * x008\n",
      "-0.227470 * x002**3*log(x003)\n",
      "0.106841 * x011\n",
      "0.080219 * x002\n",
      "0.072325 * x000\n",
      "-0.037185 * sqrt(x004)*sqrt(x006)\n",
      "-0.022055 * x009\n",
      "0.009544 * x003/x001\n",
      "-0.007095 * x006/x005\n",
      "0.006106 * exp(x011)/x001\n",
      "0.000235 * x006\n",
      "0.000177 * exp(x008)/x004\n",
      "0.000170 * sqrt(x002)*x010**3\n",
      "-0.000156 * x001*x005**2\n",
      "0.000151 * x010**3*log(x005)\n",
      "0.000131 * x003\n",
      "[AutoFeatRegression] Final R^2: 0.3484\n",
      "[AutoFeatRegression] Final dataframe with 31 feature columns (19 new).\n",
      "[AutoFeatRegression] Computing 19 new features.\n",
      "[AutoFeatRegression]    19/   19 new features ...done.\n",
      "autofeat new features: 19\n",
      "autofeat MSE on training data: 0.492586970271227\n",
      "autofeat MSE on test data: 0.5001190548173206\n",
      "autofeat R^2 on training data: 0.3483845023908111\n",
      "autofeat R^2 on test data: 0.36503553253604415\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -0.5044625309622182\n",
      "MSE on training data: 0.4923147838467054\n",
      "MSE on test data: 0.5020211111179068\n",
      "R^2 on training data: 0.3487445624475345\n",
      "R^2 on test data: 0.36262063121533894\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3891720363145036\n",
      "MSE on training data: 0.051551029440061576\n",
      "MSE on test data: 0.34983576923076926\n",
      "R^2 on training data: 0.9318060530867156\n",
      "R^2 on test data: 0.5558391931485238\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 4115 feature combinations from 1035 original feature tuples - done.\n",
      "[feateng] Generated a total of 1048 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 8/1056 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 0 features after noise filtering\n",
      "[featsel] WARNING: Not a single good features was found...\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "1.034362126350683\n",
      "0.000050 * x002\n",
      "[AutoFeatRegression] Final R^2: 0.0017\n",
      "[AutoFeatRegression] Final dataframe with 8 feature columns (0 new).\n",
      "autofeat new features: 0\n",
      "autofeat MSE on training data: 1.863966348455665\n",
      "autofeat MSE on test data: 2.302383639455436\n",
      "autofeat R^2 on training data: 0.0016934465809870236\n",
      "autofeat R^2 on test data: -0.02832219009425807\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.8969026573257473\n",
      "MSE on training data: 1.8469604643703812\n",
      "MSE on test data: 2.328893858372984\n",
      "R^2 on training data: 0.010801489514856932\n",
      "R^2 on test data: -0.0401625480215837\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.2}\n",
      "best score: -1.8836034096444383\n",
      "MSE on training data: 1.828520757317779\n",
      "MSE on test data: 2.2946977602510548\n",
      "R^2 on training data: 0.020677462012424486\n",
      "R^2 on test data: -0.024889417205844477\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 60445 features.\n",
      "[AutoFeatRegression] With 353 data points this new feature matrix would use about 0.09 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 50 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 7041 feature combinations from 1770 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 31880 transformed features from 7041 original features - done.\n",
      "[feateng] Generated a total of 33661 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 17580/33671 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 34 features after 3 feature selection runs\n",
      "[featsel] 31 features after correlation filtering\n",
      "[featsel] 7 features after noise filtering\n",
      "[featsel] 17 final features selected (including 10 original keep features).\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "261.3895537662519\n",
      "16431319033552046.000000 * x000**3*x001**9\n",
      "290.083236 * Abs(x008 + Abs(x009))\n",
      "-168.688793 * 1/(exp(x002) + Abs(x000))\n",
      "113.873188 * exp(3*x002)*exp(3*x003)\n",
      "-101.133929 * sqrt(-x008 + Abs(x008))\n",
      "-65.441176 * (x001 + exp(x006))**3\n",
      "-34.582211 * x004\n",
      "-3.645555 * x000\n",
      "0.971440 * x008/Abs(x002)\n",
      "[AutoFeatRegression] Final R^2: 0.5968\n",
      "[AutoFeatRegression] Final dataframe with 17 feature columns (7 new).\n",
      "[AutoFeatRegression] Computing 7 new features.\n",
      "[AutoFeatRegression]     7/    7 new features ...done.\n",
      "autofeat new features: 7\n",
      "autofeat MSE on training data: 2477.051664636957\n",
      "autofeat MSE on test data: 3060.39664522859\n",
      "autofeat R^2 on training data: 0.5967527393049732\n",
      "autofeat R^2 on test data: 0.39472478656779064\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1.0}\n",
      "best score: -2747.9356927470185\n",
      "MSE on training data: 2609.210829119539\n",
      "MSE on test data: 3055.4082770598457\n",
      "R^2 on training data: 0.5752381210133295\n",
      "R^2 on test data: 0.39571136966731224\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3096.0437979798735\n",
      "MSE on training data: 2234.4974811360507\n",
      "MSE on test data: 3673.923401114662\n",
      "R^2 on training data: 0.6362389201800882\n",
      "R^2 on test data: 0.2733834765469859\n",
      "#### boston\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 102466 features.\n",
      "[AutoFeatRegression] With 404 data points this new feature matrix would use about 0.17 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 64 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11659 feature combinations from 2926 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 49568 transformed features from 11659 original features - done.\n",
      "[feateng] Generated a total of 52513 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 45227/52526 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 58 features after 3 feature selection runs\n",
      "[featsel] 45 features after correlation filtering\n",
      "[featsel] 25 features after noise filtering\n",
      "[featsel] 36 final features selected (including 13 original keep features).\n",
      "[AutoFeatRegression] Computing 23 new features.\n",
      "[AutoFeatRegression]    23/   23 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "14.697415028783855\n",
      "50106457814.259560 * 1/(x009**3*x010**3)\n",
      "105.948322 * x005**6/x009**3\n",
      "53.820308 * 1/(x007 + x012)\n",
      "39.948798 * x005**3/x010**3\n",
      "10.835871 * 1/(x012*log(x007))\n",
      "-4.459194 * 1/(sqrt(x006) - x012**2)\n",
      "-3.384849 * 1/(-x002**3 + sqrt(x009))\n",
      "-3.217717 * x004**6*log(x000)**2\n",
      "-2.053410 * log(sqrt(x010) + 1/x008)\n",
      "1.650325 * sqrt(1/x009)*exp(x005/2)\n",
      "0.390649 * (-x005 + log(x009))**2\n",
      "0.382545 * 1/(log(x002)*log(x007))\n",
      "-0.323053 * (log(x012) - 1/x004)**3\n",
      "-0.080594 * 1/(log(x007) - 1/x005)\n",
      "0.068479 * exp(-sqrt(x000) + sqrt(x008))\n",
      "-0.013754 * x000\n",
      "0.011873 * (sqrt(x009) - x012)**2\n",
      "-0.011795 * x006\n",
      "-0.007770 * x012\n",
      "0.003123 * exp(x000*x003)\n",
      "0.001460 * exp(-x000**3 + x005)\n",
      "-0.000224 * sqrt(x009**3/x008)\n",
      "0.000124 * x011\n",
      "[AutoFeatRegression] Final R^2: 0.9293\n",
      "[AutoFeatRegression] Final dataframe with 36 feature columns (23 new).\n",
      "[AutoFeatRegression] Computing 23 new features.\n",
      "[AutoFeatRegression]    23/   23 new features ...done.\n",
      "autofeat new features: 23\n",
      "autofeat MSE on training data: 6.006175282331883\n",
      "autofeat MSE on test data: 78.82979725388057\n",
      "autofeat R^2 on training data: 0.929343547644375\n",
      "autofeat R^2 on test data: 0.035252580466658245\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -15.594083675804509\n",
      "MSE on training data: 5.432829201572235\n",
      "MSE on test data: 139.74373917783853\n",
      "R^2 on training data: 0.9360883724512111\n",
      "R^2 on test data: -0.7102341559190475\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.365497634166664\n",
      "MSE on training data: 1.3070924034653455\n",
      "MSE on test data: 12.3397989607843\n",
      "R^2 on training data: 0.9846234071120894\n",
      "R^2 on test data: 0.848981100805878\n",
      "#### concrete\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeatRegression] With 824 data points this new feature matrix would use about 0.13 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3422 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 11487 transformed features from 3422 original features - done.\n",
      "[feateng] Generated a total of 12360 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 11222/12368 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 51 features after 3 feature selection runs\n",
      "[featsel] 40 features after correlation filtering\n",
      "[featsel] 26 features after noise filtering\n",
      "[featsel] 33 final features selected (including 8 original keep features).\n",
      "[AutoFeatRegression] Computing 25 new features.\n",
      "[AutoFeatRegression]    25/   25 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "28.733432768401833\n",
      "-7579685956.324011 * exp(sqrt(x004) - sqrt(x006))\n",
      "-1999787365.204235 * x007*exp(-sqrt(x006))\n",
      "95955.350785 * x000/x003**3\n",
      "-23937.820614 * 1/(-x001**3 + x003**2)\n",
      "-8959.164487 * exp(-sqrt(x000) + sqrt(x004))\n",
      "-4582.581813 * exp(sqrt(x000) - sqrt(x005))\n",
      "-1737.285638 * 1/(x003**2 - x007**3)\n",
      "-1541.870657 * 1/(x000 + x001)\n",
      "-22.687824 * 1/(sqrt(x003) - x004**3)\n",
      "1.954017 * 1/(sqrt(x001) - log(x003))\n",
      "-1.694463 * Abs(sqrt(x003) - log(x007))\n",
      "-1.625193 * 1/(sqrt(x002) - log(x003))\n",
      "-1.320516 * exp(-sqrt(x000) + sqrt(x002))\n",
      "0.860073 * sqrt(x000*log(x007))\n",
      "-0.741505 * Abs(sqrt(x007) - log(x005))\n",
      "-0.740976 * exp(-sqrt(x005) + sqrt(x006))\n",
      "-0.451235 * Abs(x004 - sqrt(x007))\n",
      "-0.389337 * sqrt(sqrt(x004)*x007)\n",
      "0.312980 * x004\n",
      "-0.260225 * 1/(-x004**2 + sqrt(x005))\n",
      "0.044077 * x007\n",
      "-0.040733 * x003\n",
      "-0.028554 * x000\n",
      "0.010931 * x001\n",
      "-0.009515 * Abs(x003 - x004**2)\n",
      "0.003670 * x001*log(x007)**2\n",
      "0.003585 * x005\n",
      "0.002833 * sqrt(sqrt(x002)*x007**3)\n",
      "0.000103 * (x000 + x002)**2\n",
      "[AutoFeatRegression] Final R^2: 0.8980\n",
      "[AutoFeatRegression] Final dataframe with 33 feature columns (25 new).\n",
      "[AutoFeatRegression] Computing 25 new features.\n",
      "[AutoFeatRegression]    25/   25 new features ...done.\n",
      "autofeat new features: 25\n",
      "autofeat MSE on training data: 29.083504412884352\n",
      "autofeat MSE on test data: 36.04134044498172\n",
      "autofeat R^2 on training data: 0.8979701583427286\n",
      "autofeat R^2 on test data: 0.8579714211666739\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -35.462739938059066\n",
      "MSE on training data: 32.32135720331422\n",
      "MSE on test data: 39.55669638683277\n",
      "R^2 on training data: 0.8866112243288908\n",
      "R^2 on test data: 0.8441184123065689\n",
      "# Random Forest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -25.13205992325954\n",
      "MSE on training data: 3.9922058201135795\n",
      "MSE on test data: 22.78519350316074\n",
      "R^2 on training data: 0.9859946682522559\n",
      "R^2 on test data: 0.910210091751822\n",
      "#### airfoil\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 14910 features.\n",
      "[AutoFeatRegression] With 1202 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 509 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1906 transformed features from 509 original features - done.\n",
      "[feateng] Generated a total of 2239 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 2107/2244 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 95 features after 3 feature selection runs\n",
      "[featsel] 67 features after correlation filtering\n",
      "[featsel] 57 features after noise filtering\n",
      "[featsel] 41 final features selected (including 6 original keep features).\n",
      "[AutoFeatRegression] Computing 35 new features.\n",
      "[AutoFeatRegression]    35/   35 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "-541.6226778985962\n",
      "-56113728040.200256 * x002**6*x004**4\n",
      "2555915666.859405 * x004**9*log(x000)**3\n",
      "-2197067870.613580 * x004**3/x003**3\n",
      "358125.944338 * x002/x000**2\n",
      "-29514.648205 * x001**6*x004**9\n",
      "-2541.141123 * x001**3/x000**3\n",
      "2104.801813 * 1/(x000**3*x002**3)\n",
      "-2084.328689 * 1/(x000**2*x004)\n",
      "2050.662660 * 1/(sqrt(x000)*x003)\n",
      "762.321034 * exp(log(x002)/x000)\n",
      "204.683514 * 1/(sqrt(x000) + 1/x002)\n",
      "201.665410 * x001**(3/2)*x002**6\n",
      "-149.333892 * sqrt(x002*x004)\n",
      "-116.958159 * x001/x003**2\n",
      "99.363167 * x004\n",
      "-60.313645 * log(x000)**2/x003**2\n",
      "-40.100730 * log(exp(x002)*log(x000))\n",
      "33.935278 * x001**3*x002**6\n",
      "-24.575437 * sqrt(1/(x000*x004))\n",
      "20.207127 * x001**3/x003**3\n",
      "10.016469 * 1/(x003*sqrt(x004))\n",
      "4.272390 * x002\n",
      "1.816281 * 1/(x002**3*x003**3)\n",
      "-0.902783 * 1/(sqrt(x000) - 1/x002)\n",
      "-0.598931 * x001\n",
      "-0.525982 * sqrt(x000*x004)\n",
      "-0.323348 * x001**3/x000\n",
      "0.128440 * x003\n",
      "-0.053114 * 1/(x000*x002**3)\n",
      "-0.008419 * sqrt(x000*x001)\n",
      "-0.001349 * x000\n",
      "-0.000380 * log(x000)**3*log(x002)**3\n",
      "-0.000148 * x001**2*x003\n",
      "-0.000135 * x001**2/x004\n",
      "0.000082 * x001**9/x000**3\n",
      "0.000039 * x000**(3/2)*x002**(3/2)\n",
      "[AutoFeatRegression] Final R^2: 0.8756\n",
      "[AutoFeatRegression] Final dataframe with 41 feature columns (36 new).\n",
      "[AutoFeatRegression] Applying the Pi Theorem\n",
      "[AutoFeatRegression] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeatRegression] Computing 35 new features.\n",
      "[AutoFeatRegression]    35/   35 new features ...done.\n",
      "autofeat new features: 35\n",
      "autofeat MSE on training data: 5.834752382492857\n",
      "autofeat MSE on test data: 7.286706514766433\n",
      "autofeat R^2 on training data: 0.8755627758723071\n",
      "autofeat R^2 on test data: 0.854947442895661\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -6.829068041798915\n",
      "MSE on training data: 6.204840791140705\n",
      "MSE on test data: 7.487006494450066\n",
      "R^2 on training data: 0.8676699346281518\n",
      "R^2 on test data: 0.8509601786656306\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.864640348546687\n",
      "MSE on training data: 0.456437496277454\n",
      "MSE on test data: 3.0471395439770883\n",
      "R^2 on training data: 0.9902655997545017\n",
      "R^2 on test data: 0.9393422279582789\n",
      "#### wine_quality\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 87234 features.\n",
      "[AutoFeatRegression] With 5197 data points this new feature matrix would use about 1.81 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 63 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 11059 feature combinations from 2775 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 50887 transformed features from 11059 original features - done.\n",
      "[feateng] Generated a total of 53684 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 39646/53696 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 47 features after 3 feature selection runs\n",
      "[featsel] 39 features after correlation filtering\n",
      "[featsel] 20 features after noise filtering\n",
      "[featsel] 32 final features selected (including 12 original keep features).\n",
      "[AutoFeatRegression] Computing 20 new features.\n",
      "[AutoFeatRegression]    20/   20 new features ...done.\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "44.354226354884105\n",
      "-48.608823 * x007\n",
      "-15.929269 * 1/(x010 + 1/x001)\n",
      "9.206054 * exp(x009**2/x006)\n",
      "1.107309 * Abs(1/x005 - 1/x000)\n",
      "-1.037516 * exp(-x011)/sqrt(x005)\n",
      "-1.035155 * exp(-x011)/x005\n",
      "0.978386 * exp(log(x009)/x005)\n",
      "0.465322 * 1/(sqrt(x001) - exp(x009))\n",
      "-0.203330 * x009\n",
      "-0.165697 * log(x009)**2/x003**2\n",
      "0.148567 * x008\n",
      "-0.089626 * Abs(log(x004) + log(x006))\n",
      "-0.076833 * Abs(log(x004) + log(x005))\n",
      "0.074577 * x002\n",
      "0.056742 * (x001**3 - log(x010))**2\n",
      "0.041640 * x000\n",
      "-0.015108 * 1/(-sqrt(x001) + x003**2)\n",
      "0.008670 * x003\n",
      "-0.003990 * x005\n",
      "0.002314 * (x010 + log(x003))**2\n",
      "-0.000078 * exp(sqrt(x006) - x008**2)\n",
      "0.000057 * x003**2/x001**2\n",
      "-0.000024 * (-x005 + x008**3)**2\n",
      "[AutoFeatRegression] Final R^2: 0.3455\n",
      "[AutoFeatRegression] Final dataframe with 32 feature columns (20 new).\n",
      "[AutoFeatRegression] Computing 20 new features.\n",
      "[AutoFeatRegression]    20/   20 new features ...done.\n",
      "autofeat new features: 20\n",
      "autofeat MSE on training data: 0.49474329881793255\n",
      "autofeat MSE on test data: 0.5136000919707915\n",
      "autofeat R^2 on training data: 0.3455320170759908\n",
      "autofeat R^2 on test data: 0.347919648838827\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -0.5067112322673653\n",
      "MSE on training data: 0.4926144661201395\n",
      "MSE on test data: 0.5127379827682266\n",
      "R^2 on training data: 0.34834812968436\n",
      "R^2 on test data: 0.34901420563571284\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.38657168867994374\n",
      "MSE on training data: 0.05142718876274773\n",
      "MSE on test data: 0.34789800000000004\n",
      "R^2 on training data: 0.9319698749282224\n",
      "R^2 on test data: 0.5582994365562317\n",
      "#### forest_fires\n",
      "[AutoFeatRegression] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeatRegression] With 413 data points this new feature matrix would use about 0.06 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 38 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 4115 feature combinations from 1035 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 16839 transformed features from 4115 original features - done.\n",
      "[feateng] Generated a total of 17887 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] 8/17895 features after univariate filtering\n",
      "[featsel] Feature selection run 1/3\n",
      "[featsel] Feature selection run 2/3\n",
      "[featsel] Feature selection run 3/3\n",
      "[featsel] 0 features after noise filtering\n",
      "[featsel] WARNING: Not a single good features was found...\n",
      "[AutoFeatRegression] Training final regression model.\n",
      "[AutoFeatRegression] Trained model: largest coefficients:\n",
      "1.034362126350683\n",
      "0.000050 * x002\n",
      "[AutoFeatRegression] Final R^2: 0.0017\n",
      "[AutoFeatRegression] Final dataframe with 8 feature columns (0 new).\n",
      "autofeat new features: 0\n",
      "autofeat MSE on training data: 1.863966348455665\n",
      "autofeat MSE on test data: 2.302383639455436\n",
      "autofeat R^2 on training data: 0.0016934465809870236\n",
      "autofeat R^2 on test data: -0.02832219009425807\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 100000.0}\n",
      "best score: -1.8969026573257473\n",
      "MSE on training data: 1.8469604643703812\n",
      "MSE on test data: 2.328893858372984\n",
      "R^2 on training data: 0.010801489514856932\n",
      "R^2 on test data: -0.0401625480215837\n",
      "# Random Forest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'min_samples_leaf': 0.2}\n",
      "best score: -1.8836034096444383\n",
      "MSE on training data: 1.828520757317779\n",
      "MSE on test data: 2.2946977602510548\n",
      "R^2 on training data: 0.020677462012424486\n",
      "R^2 on test data: -0.024889417205844477\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
